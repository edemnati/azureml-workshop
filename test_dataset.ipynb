{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Manipulation using SDK V1\n",
        "\n",
        "__Make sure to select Kernel: Python 3.8 - AzureML__"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace, Dataset,Datastore, Experiment\n",
        "\n",
        "# Referencing Workspace\n",
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261310881
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Data Asset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Data file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training dataset\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "dataset_created = True\n",
        "try:\n",
        "    # checking if the directory demo_folder \n",
        "    # exist or not.\n",
        "    if not os.path.exists(\"./data\"):      \n",
        "        # if the demo_folder directory is not present \n",
        "        # then create it.\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/diabetes.csv\"\n",
        "    r = requests.get(url)\n",
        "    #retrieving data from the URL using get method\n",
        "    with open(\"./data/diabetes.csv\", 'wb') as f:    \n",
        "        f.write(r.content) \n",
        "\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    dataset_created = False\n",
        "    pass  \n"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261311388
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register Data Asset as table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "import os\n",
        "\n",
        "# Get default Datastore\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "src_dir = \"./data\"\n",
        "tgt_dir = \"/data_tables\"\n",
        "file_format=\"csv\"\n",
        "dataset_name = \"diabetes_table\"\n",
        "data_file_mask = f\"{tgt_dir}/diabetes.csv\"\n",
        "dataset_registered = True\n",
        "# Register dataset if not exists\n",
        "if dataset_name not in ws.datasets:\n",
        "    try:\n",
        "        Dataset.File.upload_directory(src_dir=src_dir,\n",
        "                            target=DataPath(default_ds, tgt_dir)\n",
        "                            )\n",
        "\n",
        "        #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
        "        tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds,data_file_mask))\n",
        "\n",
        "        # Register the tabular dataset    \n",
        "        tab_data_set = tab_data_set.register(workspace=ws, \n",
        "                                name=dataset_name,\n",
        "                                description=dataset_name,\n",
        "                                tags = {'format':file_format.upper()},\n",
        "                                create_new_version=True)\n",
        "        print('Dataset registered.')\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        dataset_registered = False\n",
        "        pass\n",
        "else:\n",
        "    print('Dataset already registered.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset already registered.\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261311759
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read a Data Asset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "dataset = Dataset.get_by_name(ws, name='diabetes_table')\n",
        "df = dataset.to_pandas_dataframe()\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "       PatientID  Pregnancies  PlasmaGlucose  DiastolicBloodPressure  \\\n0        1354778            0            171                      80   \n1        1147438            8             92                      93   \n2        1640031            7            115                      47   \n3        1883350            9            103                      78   \n4        1424119            1             85                      59   \n...          ...          ...            ...                     ...   \n14995    1490300           10             65                      60   \n14996    1744410            2             73                      66   \n14997    1742742            0             93                      89   \n14998    1099353            0            132                      98   \n14999    1386396            3            114                      65   \n\n       TricepsThickness  SerumInsulin        BMI  DiabetesPedigree  Age  \\\n0                    34            23  43.509726          1.213191   21   \n1                    47            36  21.240576          0.158365   23   \n2                    52            35  41.511523          0.079019   23   \n3                    25           304  29.582192          1.282870   43   \n4                    27            35  42.604536          0.549542   22   \n...                 ...           ...        ...               ...  ...   \n14995                46           177  33.512468          0.148327   41   \n14996                27           168  30.132636          0.862252   38   \n14997                43            57  18.690683          0.427049   24   \n14998                18           161  19.791645          0.302257   23   \n14999                47           512  36.215437          0.147363   34   \n\n       Diabetic  \n0             0  \n1             0  \n2             0  \n3             1  \n4             0  \n...         ...  \n14995         1  \n14996         1  \n14997         0  \n14998         0  \n14999         1  \n\n[15000 rows x 10 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PatientID</th>\n      <th>Pregnancies</th>\n      <th>PlasmaGlucose</th>\n      <th>DiastolicBloodPressure</th>\n      <th>TricepsThickness</th>\n      <th>SerumInsulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigree</th>\n      <th>Age</th>\n      <th>Diabetic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1354778</td>\n      <td>0</td>\n      <td>171</td>\n      <td>80</td>\n      <td>34</td>\n      <td>23</td>\n      <td>43.509726</td>\n      <td>1.213191</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1147438</td>\n      <td>8</td>\n      <td>92</td>\n      <td>93</td>\n      <td>47</td>\n      <td>36</td>\n      <td>21.240576</td>\n      <td>0.158365</td>\n      <td>23</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1640031</td>\n      <td>7</td>\n      <td>115</td>\n      <td>47</td>\n      <td>52</td>\n      <td>35</td>\n      <td>41.511523</td>\n      <td>0.079019</td>\n      <td>23</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1883350</td>\n      <td>9</td>\n      <td>103</td>\n      <td>78</td>\n      <td>25</td>\n      <td>304</td>\n      <td>29.582192</td>\n      <td>1.282870</td>\n      <td>43</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1424119</td>\n      <td>1</td>\n      <td>85</td>\n      <td>59</td>\n      <td>27</td>\n      <td>35</td>\n      <td>42.604536</td>\n      <td>0.549542</td>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14995</th>\n      <td>1490300</td>\n      <td>10</td>\n      <td>65</td>\n      <td>60</td>\n      <td>46</td>\n      <td>177</td>\n      <td>33.512468</td>\n      <td>0.148327</td>\n      <td>41</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14996</th>\n      <td>1744410</td>\n      <td>2</td>\n      <td>73</td>\n      <td>66</td>\n      <td>27</td>\n      <td>168</td>\n      <td>30.132636</td>\n      <td>0.862252</td>\n      <td>38</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14997</th>\n      <td>1742742</td>\n      <td>0</td>\n      <td>93</td>\n      <td>89</td>\n      <td>43</td>\n      <td>57</td>\n      <td>18.690683</td>\n      <td>0.427049</td>\n      <td>24</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14998</th>\n      <td>1099353</td>\n      <td>0</td>\n      <td>132</td>\n      <td>98</td>\n      <td>18</td>\n      <td>161</td>\n      <td>19.791645</td>\n      <td>0.302257</td>\n      <td>23</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14999</th>\n      <td>1386396</td>\n      <td>3</td>\n      <td>114</td>\n      <td>65</td>\n      <td>47</td>\n      <td>512</td>\n      <td>36.215437</td>\n      <td>0.147363</td>\n      <td>34</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>15000 rows × 10 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1695261312065
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register Data Asset as Folder"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "ds = Datastore(ws, \"ezmylake\")\n",
        "\n",
        "# create input dataset\n",
        "Dataset.File.upload_directory(src_dir=f\"../data/\",\n",
        "                            target=DataPath(ds, f\"./data/\")\n",
        "                            )\n",
        "\n",
        "data = Dataset.File.from_files(path=(ds, \"data/*\"))\n",
        "folder_dataset = data.register(workspace=ws, name=\"test_dataset_files\",create_new_version=True)\n",
        "\n",
        "folder_dataset.to_path()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validating arguments.\nArguments validated.\nUploading file to ./data/\nUploading an estimated of 4 files\nTarget already exists. Skipping upload for data/.amlignore\nTarget already exists. Skipping upload for data/.amlignore.amltmp\nTarget already exists. Skipping upload for data/diabetes.csv\nTarget already exists. Skipping upload for data/diabetes_2.csv\nUploaded 0 files\nCreating new dataset\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "['/.amlignore',\n '/.amlignore.amltmp',\n '/NYCTripSmall.parquet',\n '/daily-bike-share.csv',\n '/daily-bike-share.json',\n '/daily-bike-share_corrupted.csv',\n '/diabetes.csv',\n '/diabetes_2.csv',\n '/edc_eventcal_APR_sample500.json',\n '/eventcal_data_20200101_20211231.json',\n '/form_text1.png',\n '/test.csv',\n '/test.jsonl',\n '/test_json.json',\n '/test_json_array.json',\n '/test_json_array_empty.json',\n '/test_json_array_nested.json',\n '/test_json_empty.json']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261314057
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter files based on file extension"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only on CSV files\n",
        "test_res=folder_dataset.filter(folder_dataset.file_metadata('Extension').ends_with('csv'))\n",
        "\n",
        "# Show selected csv files\n",
        "print(test_res.to_path())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Method file_metadata: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nMethod filter: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Resolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n['/daily-bike-share.csv', '/daily-bike-share_corrupted.csv', '/diabetes.csv', '/diabetes_2.csv', '/test.csv']\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261314399
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data files from blob storage"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "default_ds = Datastore(ws, \"ezmylake\") #ws.get_default_datastore()\n",
        "dataset = Dataset.Tabular.from_delimited_files(path = [(default_ds, 'data/daily-bike-share.csv')])\n",
        "\n",
        "# preview the first 3 rows of the dataset\n",
        "dataset.take(3).to_pandas_dataframe()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Resolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "   day  mnth  year  season  holiday  weekday  workingday  weathersit  \\\n0    1     1  2011       1        0        6           0           2   \n1    2     1  2011       1        0        0           0           2   \n2    3     1  2011       1        0        1           1           1   \n\n       temp     atemp       hum  windspeed  rentals  \n0  0.344167  0.363625  0.805833   0.160446      331  \n1  0.363478  0.353739  0.696087   0.248539      131  \n2  0.196364  0.189405  0.437273   0.248309      120  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>mnth</th>\n      <th>year</th>\n      <th>season</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>rentals</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.344167</td>\n      <td>0.363625</td>\n      <td>0.805833</td>\n      <td>0.160446</td>\n      <td>331</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.363478</td>\n      <td>0.353739</td>\n      <td>0.696087</td>\n      <td>0.248539</td>\n      <td>131</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.196364</td>\n      <td>0.189405</td>\n      <td>0.437273</td>\n      <td>0.248309</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261314769
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Read multiple files as files__"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datastore = Datastore.get(ws, 'ezmylake')\n",
        "\n",
        "#Create a file dataset from the path on the datastore (this may take a short while)\n",
        "file_data_set = Dataset.File.from_files(path=(datastore, 'data/*.csv'))\n",
        "res=file_data_set.download()\n",
        "print(res)\n",
        "# Get the files in the dataset\n",
        "for file_path in file_data_set.to_path():\n",
        "    print(file_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Resolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n['/tmp/tmpba6ziay1/daily-bike-share.csv', '/tmp/tmpba6ziay1/diabetes.csv', '/tmp/tmpba6ziay1/test.csv', '/tmp/tmpba6ziay1/daily-bike-share_corrupted.csv', '/tmp/tmpba6ziay1/diabetes_2.csv']\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n/daily-bike-share.csv\n/daily-bike-share_corrupted.csv\n/diabetes.csv\n/diabetes_2.csv\n/test.csv\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261315175
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Read files\n",
        "df = pd.read_csv(res[0])\n",
        "print(len(df))\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "731\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "     day  mnth  year  season  holiday  weekday  workingday  weathersit  \\\n0      1     1  2011       1        0        6           0           2   \n1      2     1  2011       1        0        0           0           2   \n2      3     1  2011       1        0        1           1           1   \n3      4     1  2011       1        0        2           1           1   \n4      5     1  2011       1        0        3           1           1   \n..   ...   ...   ...     ...      ...      ...         ...         ...   \n726   27    12  2012       1        0        4           1           2   \n727   28    12  2012       1        0        5           1           2   \n728   29    12  2012       1        0        6           0           2   \n729   30    12  2012       1        0        0           0           1   \n730   31    12  2012       1        0        1           1           2   \n\n         temp     atemp       hum  windspeed  rentals  \n0    0.344167  0.363625  0.805833   0.160446      331  \n1    0.363478  0.353739  0.696087   0.248539      131  \n2    0.196364  0.189405  0.437273   0.248309      120  \n3    0.200000  0.212122  0.590435   0.160296      108  \n4    0.226957  0.229270  0.436957   0.186900       82  \n..        ...       ...       ...        ...      ...  \n726  0.254167  0.226642  0.652917   0.350133      247  \n727  0.253333  0.255046  0.590000   0.155471      644  \n728  0.253333  0.242400  0.752917   0.124383      159  \n729  0.255833  0.231700  0.483333   0.350754      364  \n730  0.215833  0.223487  0.577500   0.154846      439  \n\n[731 rows x 13 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>mnth</th>\n      <th>year</th>\n      <th>season</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>rentals</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.344167</td>\n      <td>0.363625</td>\n      <td>0.805833</td>\n      <td>0.160446</td>\n      <td>331</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.363478</td>\n      <td>0.353739</td>\n      <td>0.696087</td>\n      <td>0.248539</td>\n      <td>131</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.196364</td>\n      <td>0.189405</td>\n      <td>0.437273</td>\n      <td>0.248309</td>\n      <td>120</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.200000</td>\n      <td>0.212122</td>\n      <td>0.590435</td>\n      <td>0.160296</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.226957</td>\n      <td>0.229270</td>\n      <td>0.436957</td>\n      <td>0.186900</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>726</th>\n      <td>27</td>\n      <td>12</td>\n      <td>2012</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.254167</td>\n      <td>0.226642</td>\n      <td>0.652917</td>\n      <td>0.350133</td>\n      <td>247</td>\n    </tr>\n    <tr>\n      <th>727</th>\n      <td>28</td>\n      <td>12</td>\n      <td>2012</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.253333</td>\n      <td>0.255046</td>\n      <td>0.590000</td>\n      <td>0.155471</td>\n      <td>644</td>\n    </tr>\n    <tr>\n      <th>728</th>\n      <td>29</td>\n      <td>12</td>\n      <td>2012</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.253333</td>\n      <td>0.242400</td>\n      <td>0.752917</td>\n      <td>0.124383</td>\n      <td>159</td>\n    </tr>\n    <tr>\n      <th>729</th>\n      <td>30</td>\n      <td>12</td>\n      <td>2012</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.255833</td>\n      <td>0.231700</td>\n      <td>0.483333</td>\n      <td>0.350754</td>\n      <td>364</td>\n    </tr>\n    <tr>\n      <th>730</th>\n      <td>31</td>\n      <td>12</td>\n      <td>2012</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.215833</td>\n      <td>0.223487</td>\n      <td>0.577500</td>\n      <td>0.154846</td>\n      <td>439</td>\n    </tr>\n  </tbody>\n</table>\n<p>731 rows × 13 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261316707
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Read multiple files as tables__"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "# Get the default datastore\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
        "tab_data_set = Dataset.Tabular.from_delimited_files(path=(datastore, 'data/daily-bike-share*.csv'))\n",
        "#tab_data_set.get_all()\n",
        "# Display the first 20 rows as a Pandas dataframe\n",
        "df2 = tab_data_set.to_pandas_dataframe()\n",
        "print(len(df2))\n",
        "df2"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Resolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n1463\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "       day  mnth    year  season  holiday  weekday  workingday  weathersit  \\\n0      1.0   1.0  2011.0     1.0      0.0      6.0         0.0         2.0   \n1      2.0   1.0  2011.0     1.0      0.0      0.0         0.0         2.0   \n2      3.0   1.0  2011.0     1.0      0.0      1.0         1.0         1.0   \n3      4.0   1.0  2011.0     1.0      0.0      2.0         1.0         1.0   \n4      5.0   1.0  2011.0     1.0      0.0      3.0         1.0         1.0   \n...    ...   ...     ...     ...      ...      ...         ...         ...   \n1458  28.0  12.0  2012.0     1.0      0.0      5.0         1.0         2.0   \n1459  29.0  12.0  2012.0     1.0      0.0      6.0         0.0         2.0   \n1460  30.0  12.0  2012.0     1.0      0.0      0.0         0.0         1.0   \n1461  31.0  12.0  2012.0     1.0      0.0      1.0         1.0         2.0   \n1462   NaN   NaN     NaN     NaN      NaN      NaN         NaN         NaN   \n\n          temp     atemp       hum  windspeed  rentals  \n0     0.344167  0.363625  0.805833   0.160446    331.0  \n1     0.363478  0.353739  0.696087   0.248539    131.0  \n2     0.196364  0.189405  0.437273   0.248309    120.0  \n3     0.200000  0.212122  0.590435   0.160296    108.0  \n4     0.226957  0.229270  0.436957   0.186900     82.0  \n...        ...       ...       ...        ...      ...  \n1458  0.253333  0.255046  0.590000   0.155471    644.0  \n1459  0.253333  0.242400  0.752917   0.124383    159.0  \n1460  0.255833  0.231700  0.483333   0.350754    364.0  \n1461  0.215833  0.223487  0.577500   0.154846    439.0  \n1462       NaN       NaN       NaN        NaN      NaN  \n\n[1463 rows x 13 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>mnth</th>\n      <th>year</th>\n      <th>season</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>rentals</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2011.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.344167</td>\n      <td>0.363625</td>\n      <td>0.805833</td>\n      <td>0.160446</td>\n      <td>331.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2011.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.363478</td>\n      <td>0.353739</td>\n      <td>0.696087</td>\n      <td>0.248539</td>\n      <td>131.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2011.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.196364</td>\n      <td>0.189405</td>\n      <td>0.437273</td>\n      <td>0.248309</td>\n      <td>120.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2011.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.200000</td>\n      <td>0.212122</td>\n      <td>0.590435</td>\n      <td>0.160296</td>\n      <td>108.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>2011.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.226957</td>\n      <td>0.229270</td>\n      <td>0.436957</td>\n      <td>0.186900</td>\n      <td>82.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>28.0</td>\n      <td>12.0</td>\n      <td>2012.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.253333</td>\n      <td>0.255046</td>\n      <td>0.590000</td>\n      <td>0.155471</td>\n      <td>644.0</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>29.0</td>\n      <td>12.0</td>\n      <td>2012.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.253333</td>\n      <td>0.242400</td>\n      <td>0.752917</td>\n      <td>0.124383</td>\n      <td>159.0</td>\n    </tr>\n    <tr>\n      <th>1460</th>\n      <td>30.0</td>\n      <td>12.0</td>\n      <td>2012.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.255833</td>\n      <td>0.231700</td>\n      <td>0.483333</td>\n      <td>0.350754</td>\n      <td>364.0</td>\n    </tr>\n    <tr>\n      <th>1461</th>\n      <td>31.0</td>\n      <td>12.0</td>\n      <td>2012.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.215833</td>\n      <td>0.223487</td>\n      <td>0.577500</td>\n      <td>0.154846</td>\n      <td>439.0</td>\n    </tr>\n    <tr>\n      <th>1462</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1463 rows × 13 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695261317105
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create pipeline and pass data assets as input"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dummy_train.py\n",
        "\n",
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# Licensed under the MIT License.\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from azureml.core import Run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "print(\"*********************************************************\")\n",
        "print(\"Hello Azure Machine Learning!\")\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"process\")\n",
        "\n",
        " \n",
        "\n",
        "parser.add_argument(\"--input_ds\", type=str, dest='input_data', help=\"input scored data\")\n",
        "\n",
        "parser.add_argument(\"--output_ds\", type=str, dest='save_folder', help=\"output directory for GW data\")\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "#print(\"Argument 1: %s\" % args.input_ds)\n",
        "print(\"Argument 1 bis: %s\" % args.input_data)\n",
        "print(\"Argument 2: %s\" % args.save_folder)\n",
        "print(f'list dir:\\n{os.listdir(args.save_folder)}')\n",
        "\n",
        "print(\"Loading Data...\")\n",
        "#used with input parameter: dataset_raw_data\n",
        "diabetes = run.input_datasets['input'].to_pandas_dataframe()\n",
        "\n",
        "#used with input parameter: file_data_set\n",
        "#diabetes = pd.read_csv(args.input_data)\n",
        "\n",
        "#used with input parameter: dataset_files\n",
        "#diabetes = pd.read_csv(os.path.join(args.input_data,\"daily-bike-share.csv\"))\n",
        "\n",
        "#-----------------------#\n",
        "\n",
        "# INSERT your model training code here\n",
        "\n",
        "#-----------------------#\n",
        "\n",
        "#logging\n",
        "run.log(\"accuracy\", 0.95)\n",
        "run.log_list(\"accuracies\", [0.6, 0.7, 0.87])\n",
        "run.log_row(\"Y over X\", x=1, y=0.4)\n",
        "run.log_table(\"Y over X table\", {\"x\":[1, 2, 3], \"y\":[0.6, 0.7, 0.89]})\n",
        "\n",
        "#run.log_image(\"ROC\", path)\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.title(\"test log image)\", fontsize=14)\n",
        "plt.plot([0.6, 0.7, 0.87], \"b-\", label=\"Accuracy\", lw=4, alpha=0.5)\n",
        "plt.plot([0.1, 0.2, 0.5], \"r--\", label=\"Loss\", lw=4, alpha=0.5)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True)\n",
        "run.log_image(\"Loss v.s. Accuracy\", plot=plt)\n",
        "\n",
        "\n",
        "print(f\"df len:{len(diabetes)}\")\n",
        "\n",
        "diabetes.head(10).to_csv(os.path.join(args.save_folder, 'output2_10.csv'))\n",
        "diabetes.head(100).to_csv(os.path.join(args.save_folder, 'output2_100.csv'))\n",
        "diabetes.to_csv(os.path.join(args.save_folder, 'output2_all.csv'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting dummy_train.py\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile env.yml\n",
        "\n",
        "name: simple_environment\n",
        "dependencies:\n",
        "  # The python interpreter version.\n",
        "  # Currently Azure ML only supports 3.5.2 and later.\n",
        "- python=3.8.3\n",
        "- scikit-learn\n",
        "- pandas\n",
        "- pip\n",
        "- pip:\n",
        "  - azureml-defaults\n",
        "  - azureml-mlflow\n",
        "  - pandas\n",
        "  - numpy\n",
        "  - joblib\n",
        "  - scikit-learn\n",
        "  - matplotlib\n",
        "  - seaborn"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting env.yml\n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# Get or create registered environment\n",
        "name = \"experiment_env\"\n",
        "file_path = r\"env.yml\"\n",
        "environment_registered = True\n",
        "registered_env = None\n",
        "try:\n",
        "    #register environement if not exist\n",
        "    env_names = Environment.list(workspace=ws)\n",
        "    current_env = name\n",
        "    if current_env in env_names:\n",
        "        registered_env = Environment.get(ws, current_env)\n",
        "    else:    \n",
        "        env = Environment.from_conda_specification(name=name, file_path=file_path)\n",
        "\n",
        "        # Registering and reusing environments\n",
        "        registered_env = env.register(workspace=ws)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    environment_registered = False\n",
        "    pass\n",
        "\n",
        "print(registered_env)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment(Name: experiment_env,\nVersion: 8)\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1695261317780
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "# Get the default datastore\n",
        "datastore = Datastore.get(ws, 'ezmylake')\n",
        "\n",
        "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
        "tab_data_set = Dataset.Tabular.from_delimited_files(path=(datastore, 'data/*.csv'))\n",
        "\n",
        "# Display the first 20 rows as a Pandas dataframe\n",
        "#tab_data_set.take(20).to_pandas_dataframe().head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Resolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nResolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n"
        }
      ],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1695261318282
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig\n",
        "from azureml.data.datapath import DataPath\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "\n",
        "#input_data= mnist_ds.as_named_input('input').as_mount()# the dataset will be mounted on the remote compute \n",
        "\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute, ComputeInstance\n",
        "# Get compute instance to run a test\n",
        "ci_instance = ComputeInstance(workspace=ws, name=\"re-compute-inst\")\n",
        "compute_target = ComputeTarget(workspace=ws, name=\"re-compute-cl\")\n",
        "\n",
        "experiment = Experiment(workspace = ws, name = 'test-ds-pipeline') #regenerate_outputs=True \n",
        "\n",
        "datastore = Datastore.get(ws, 'ezmylake')\n",
        "#Inputs\n",
        "\"\"\"\n",
        "Once you've created a named input, you can choose its access mode: as_mount() or as_download(). \n",
        "  - If your script processes all the files in your dataset and the disk on your compute resource is large enough for the dataset, \n",
        "    the download access mode is the better choice. The download access mode avoids the overhead of streaming the data at runtime. \n",
        "  - If your script accesses a subset of the dataset or it's too large for your compute, use the mount access mode. \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#Tabular: input_dataset.as_named_input('input')\n",
        "input_dataset = Dataset.get_by_name(ws, 'diabetes.csv') \n",
        "input_dataset = ws.datasets.get(\"diabetes.csv\") \n",
        "\n",
        "#Files: input_file_dataset.as_named_input('input').as_mount()\n",
        "input_file_dataset = ws.datasets.get(\"test file dataset\") #Reference a folder\n",
        "input_file_dataset = Dataset.File.from_files(path=(datastore, 'data/daily-bike-share.csv')) #Reference a specific file in a folder\n",
        "\n",
        "#Outputs\n",
        "\"\"\"\n",
        "Ref: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines?view=azureml-api-1\n",
        "OutputFileDatasetConfig supports writing data to blob storage, fileshare, adlsgen1, or adlsgen2. \n",
        "It supports both mount mode and upload mode. \n",
        "  - In mount mode, files written to the mounted directory are permanently stored when the file is closed. \n",
        "  - In upload mode, files written to the output directory are uploaded at the end of the job. \n",
        "    If the job fails or is canceled, the output directory won't be uploaded.\n",
        "\"\"\"\n",
        "output = OutputFileDatasetConfig(destination=(datastore, 'sample/outputdataset2')) #Output to blob storage\n",
        "\n",
        "#prepared_output_ds = (OutputFileDatasetConfig(destination=(datastore, 'outputdataset'))\n",
        "#                      .register_on_complete(name='prepared_fashion_ds')\n",
        "#                     ) #Output to blob storage and register data asset\n",
        "#step1_output_data = (OutputFileDatasetConfig(name=\"processed_data_mount\", \n",
        "#                        destination=(datastore, \"sample/outputdataset2/{run-id}/{output-name}\")).as_mount()\n",
        "#                    )\n",
        "\n",
        "#Job config\n",
        "src = ScriptRunConfig(source_directory='./',\n",
        "                      script='dummy_train.py',\n",
        "                      arguments=[\"--input_ds\", input_dataset.as_named_input('input'), \n",
        "                                 \"--output_ds\",output],\n",
        "                      compute_target=ci_instance,\n",
        "                      environment=registered_env)\n",
        "\n",
        "# Submit the run configuration for your training run\n",
        "run = experiment.submit(src)\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Resolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\nRunId: test-ds-pipeline_1695261658_10fd6429\nWeb View: https://ml.azure.com/runs/test-ds-pipeline_1695261658_10fd6429?wsid=/subscriptions/e0d7a68e-191f-4f51-83ce-d93995cd5c09/resourcegroups/my_ml_tests/workspaces/myworkspace&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n\nStreaming user_logs/std_log.txt\n===============================\n\n*********************************************************\nHello Azure Machine Learning!\nArgument 1 bis: b06fa5c4-9802-46d5-a96a-56662060b41c\nArgument 2: /mnt/azureml/cr/j/ba3696e69e3745c3aed7ac8170972ab6/cap/data-capability/wd/output_43061f8a\nlist dir:\n['output.csv', 'output10.csv', 'output100.csv', 'output2_10.csv', 'output2_100.csv', 'output2_all.csv', 'output3_10.csv', 'output3_100.csv', 'output3_all.csv', 'output_all.csv', 'test-ds-pipeline_1690808620_35f01228', 'test-ds-pipeline_1690808736_cb41d463', 'test-ds-pipeline_1690808817_7427836b', 'test-ds-pipeline_1690808941_3d0a3aa9', 'test-ds-pipeline_1690809020_c66a704b', 'test-ds-pipeline_1690809089_727c555a', 'test-ds-pipeline_1690809792_d51f4c9a', 'test-ds-pipeline_1690809981_ab4b17fb', 'test-ds-pipeline_1690810058_1b2f339c', 'test-ds-pipeline_1695261317_2af5c3cd']\nLoading Data...\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/bin/bash: /azureml-envs/azureml_c4237f9a703ec2e2d6eb805845fb9607/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nFailed to extract subscription information, Exception=AttributeError; 'Logger' object has no attribute 'activity_info'\nFailed to extract subscription information, Exception=AttributeError; 'Logger' object has no attribute 'activity_info'\nFailed to extract subscription information, Exception=AttributeError; 'Logger' object has no attribute 'activity_info'\nFailed to extract subscription information, Exception=AttributeError; 'Logger' object has no attribute 'activity_info'\nFailed to extract subscription information, Exception=AttributeError; 'Logger' object has no attribute 'activity_info'\nFailed to extract subscription information, Exception=AttributeError; 'Logger' object has no attribute 'activity_info'\ndf len:15000\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n1 items cleaning up...\nCleanup took 5.3863842487335205 seconds\n\nExecution Summary\n=================\nRunId: test-ds-pipeline_1695261658_10fd6429\nWeb View: https://ml.azure.com/runs/test-ds-pipeline_1695261658_10fd6429?wsid=/subscriptions/e0d7a68e-191f-4f51-83ce-d93995cd5c09/resourcegroups/my_ml_tests/workspaces/myworkspace&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "{'runId': 'test-ds-pipeline_1695261658_10fd6429',\n 'target': 're-compute-inst',\n 'status': 'Completed',\n 'startTimeUtc': '2023-09-21T02:01:13.458824Z',\n 'endTimeUtc': '2023-09-21T02:01:47.664842Z',\n 'services': {},\n 'properties': {'_azureml.ComputeTargetType': 'amlcdsi',\n  'ContentSnapshotId': '5b4ac3fd-1073-4039-9d33-ea5bc5b0a870',\n  'ProcessInfoFile': 'azureml-logs/process_info.json',\n  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n 'inputDatasets': [{'dataset': {'id': 'b06fa5c4-9802-46d5-a96a-56662060b41c'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'input', 'mechanism': 'Direct'}}],\n 'outputDatasets': [{'identifier': {'savedId': '2167dfed-a5c7-4445-a98b-030c6f5bff63'},\n   'outputType': 'RunOutput',\n   'outputDetails': {'outputName': 'output_43061f8a'},\n   'dataset': {\n     \"source\": [\n       \"('ezmylake', 'sample/outputdataset2')\"\n     ],\n     \"definition\": [\n       \"GetDatastoreFiles\"\n     ],\n     \"registration\": {\n       \"id\": \"2167dfed-a5c7-4445-a98b-030c6f5bff63\",\n       \"name\": null,\n       \"version\": null,\n       \"workspace\": \"Workspace.create(name='myworkspace', subscription_id='e0d7a68e-191f-4f51-83ce-d93995cd5c09', resource_group='my_ml_tests')\"\n     }\n   }}],\n 'runDefinition': {'script': 'dummy_train.py',\n  'command': '',\n  'useAbsolutePath': False,\n  'arguments': ['--input_ds',\n   'DatasetConsumptionConfig:input',\n   '--output_ds',\n   'DatasetOutputConfig:output_43061f8a'],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 're-compute-inst',\n  'dataReferences': {},\n  'data': {'input': {'dataLocation': {'dataset': {'id': 'b06fa5c4-9802-46d5-a96a-56662060b41c',\n      'name': 'diabetes.csv',\n      'version': '1'},\n     'dataPath': None,\n     'uri': None,\n     'type': None},\n    'mechanism': 'Direct',\n    'environmentVariableName': 'input',\n    'pathOnCompute': None,\n    'overwrite': False,\n    'options': None}},\n  'outputData': {'output_43061f8a': {'outputLocation': {'dataset': None,\n     'dataPath': {'datastoreName': 'ezmylake',\n      'relativePath': 'sample/outputdataset2'},\n     'uri': None,\n     'type': None},\n    'mechanism': 'Mount',\n    'additionalOptions': {'pathOnCompute': None,\n     'registrationOptions': {'name': None,\n      'description': None,\n      'tags': None,\n      'datasetRegistrationOptions': {'additionalTransformation': None}},\n     'uploadOptions': None,\n     'mountOptions': {'disableMetadataCache': 'False'}},\n    'environmentVariableName': None}},\n  'datacaches': [],\n  'jobName': None,\n  'maxRunDurationSeconds': 2592000,\n  'nodeCount': 1,\n  'instanceTypes': [],\n  'priority': None,\n  'credentialPassthrough': False,\n  'identity': None,\n  'environment': {'name': 'experiment_env',\n   'version': '8',\n   'assetId': 'azureml://locations/eastus2/workspaces/abb11f9d-372a-40d8-9fb1-3ce491d70f6d/environments/experiment_env/versions/8',\n   'autoRebuild': True,\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'name': 'simple_environment',\n     'dependencies': ['python=3.8.3',\n      'scikit-learn',\n      'pandas',\n      'pip',\n      {'pip': ['azureml-defaults',\n        'azureml-mlflow',\n        'pandas',\n        'numpy',\n        'joblib',\n        'scikit-learn',\n        'matplotlib',\n        'seaborn']}]},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20220729.v1',\n    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n    'baseDockerfile': None,\n    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n    'enabled': False,\n    'arguments': []},\n   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n   'inferencingStackVersion': None},\n  'history': {'outputCollection': True,\n   'directoriesToWatch': ['logs'],\n   'enableMLflowTracking': True,\n   'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'parallelTask': {'maxRetriesPerWorker': 0,\n   'workerCountPerNode': 1,\n   'terminalExitCodes': None,\n   'configuration': {}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': None},\n  'aiSuperComputer': {'instanceType': 'D2',\n   'imageVersion': None,\n   'location': None,\n   'aiSuperComputerStorageData': None,\n   'interactive': False,\n   'scalePolicy': None,\n   'virtualClusterArmId': None,\n   'tensorboardLogDirectory': None,\n   'sshPublicKey': None,\n   'sshPublicKeys': None,\n   'enableAzmlInt': True,\n   'priority': 'Medium',\n   'slaTier': 'Standard',\n   'userAlias': None},\n  'kubernetesCompute': {'instanceType': None},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n  'exposedPorts': None,\n  'docker': {'useDocker': False,\n   'sharedVolumes': True,\n   'shmSize': '2g',\n   'arguments': []},\n  'cmk8sCompute': {'configuration': {}},\n  'commandReturnCodeConfig': {'returnCode': 'Zero',\n   'successfulReturnCodes': []},\n  'environmentVariables': {},\n  'applicationEndpoints': {},\n  'parameters': []},\n 'logFiles': {'logs/azureml/dataprep/0/backgroundProcess.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/logs/azureml/dataprep/0/backgroundProcess.log?sv=2019-07-07&sr=b&sig=r3UG2T0YOhGJfKa0ei9%2F9Oj47T0wTeuZkxiKgcksRCQ%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A42Z&se=2023-09-21T10%3A01%3A42Z&sp=r',\n  'logs/azureml/dataprep/0/backgroundProcess_Telemetry.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/logs/azureml/dataprep/0/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=mWdxAsbPp7%2BsDwWxkiiOXf6IBVidlppQfVRLmdkQpkA%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A42Z&se=2023-09-21T10%3A01%3A42Z&sp=r',\n  'logs/azureml/dataprep/0/rslex.log.2023-09-21-02': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/logs/azureml/dataprep/0/rslex.log.2023-09-21-02?sv=2019-07-07&sr=b&sig=G3JrkS8gQBCP7mykloxIBspWvZzFNAbQlbZmhDZT5ys%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A42Z&se=2023-09-21T10%3A01%3A42Z&sp=r',\n  'user_logs/std_log.txt': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=jBOece2Re5CulyKlyRL60J2QlageWJI4h5%2FTYALNk0Q%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/cs_capability/cs-capability.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=ym8QCiIyXn0%2F7%2F%2Bm0CU%2BCTggQmh%2BCLNTEt6W5iVqr5Q%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/data_capability/data-capability.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/data_capability/data-capability.log?sv=2019-07-07&sr=b&sig=zvxrkmUuXt3bTjL2wGS27F%2BFDIpQB%2FAoiqJiSnJHcpo%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/data_capability/rslex.log.2023-09-21-02': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/data_capability/rslex.log.2023-09-21-02?sv=2019-07-07&sr=b&sig=RZjvED0mRBAl9HgLEyHA4qKLZu%2BMQBrbOnH7jW8lKz8%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/hosttools_capability/hosttools-capability.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=3oVNjozc%2FbyS3KZ%2FefhvpTZl4vebr42HgyQc3atwt70%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/lifecycler/execution-wrapper.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=JVVyQZrk6GLde46P9j0c2DIdrTwBjKgDEGQRcvq1d%2BI%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/lifecycler/lifecycler.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=oH%2BBYWL8KNCRrv2jzlEQLDzztDDj%2FGHk6Un5Q8ojgME%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/lifecycler/vm-bootstrapper.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/lifecycler/vm-bootstrapper.log?sv=2019-07-07&sr=b&sig=umtSeQ%2FHz3wOcqQrgMptcme5ku%2FAUNWhmiIVIt52dnw%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/metrics_capability/metrics-capability.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=WnYl3jjb%2F%2Fv7k1Z0kRAYWFpRZUY0FWBjU6ipzMXyiD4%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r',\n  'system_logs/snapshot_capability/snapshot-capability.log': 'https://myworkspstorage52299cf2a.blob.core.windows.net/azureml/ExperimentRun/dcid.test-ds-pipeline_1695261658_10fd6429/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=fGtPv2G9RL1AoGckjfZKyJkDLEFBFxr%2FZrso7gCyRyE%3D&skoid=70a9bea3-8728-420a-ad85-1341717c32f3&sktid=16b3c013-d300-468d-ac64-7eda0820b6d3&skt=2023-09-21T01%3A08%3A18Z&ske=2023-09-22T09%3A18%3A18Z&sks=b&skv=2019-07-07&st=2023-09-21T01%3A51%3A54Z&se=2023-09-21T10%3A01%3A54Z&sp=r'},\n 'submittedBy': 'Ezzat Demnati'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1695261715928
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}